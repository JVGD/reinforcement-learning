{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy iteration\n",
    "\n",
    "Policy Iteration (PI), Howard (1960), iterates between the policy evaluation phase, that computes the value function of the current policy, and the policy improvement phase, that computes an improved policy by a maximization over the value function. This is repeated until converging to an optimal policy.\n",
    "\n",
    "Consider grid below:\n",
    "\n",
    "![Example 4.1](images/example_4_1_sutton_barto.png)\n",
    "\n",
    "The nonterminal states are $S = {1, 2, . . . , 14}$. There are four actions possible in each state, $A = \\left\\{ \\textrm{up, down, right, left} \\right\\}$, which deterministically cause the corresponding state transitions, except that actions that would take the agent off the grid in fact leave the state unchanged. \n",
    "\n",
    "The reward is $−1$ on all transitions until the terminal state is reached. The terminal state is shaded in the figure (although it is shown in two places, it is formally one state). The expected reward function is thus $r(s, a, s') = −1$ for all states $s, s'$ and actions $a$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting environment set up\n",
    "\n",
    "Setting up the grid, number of states, rewards for every state..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pi initial policy:\n",
      "[[0.21377114 0.32625665 0.28662183 0.17335038]\n",
      " [0.01838916 0.19805375 0.39424971 0.38930738]\n",
      " [0.27214618 0.14436676 0.43832854 0.14515852]\n",
      " [0.34497464 0.01837791 0.30167073 0.33497671]\n",
      " [0.21928808 0.00252043 0.30156304 0.47662844]\n",
      " [0.30203349 0.31495458 0.33786305 0.04514888]\n",
      " [0.2382511  0.35971708 0.27779511 0.1242367 ]\n",
      " [0.22860847 0.08490726 0.07823468 0.60824959]\n",
      " [0.34258895 0.60995404 0.00368653 0.04377047]\n",
      " [0.31533965 0.38397744 0.23513801 0.0655449 ]\n",
      " [0.37033125 0.00426326 0.42087954 0.20452596]\n",
      " [0.44315223 0.07165464 0.36574997 0.11944317]\n",
      " [0.35213516 0.27373723 0.04187704 0.33225058]\n",
      " [0.33903907 0.0589333  0.2193766  0.38265103]\n",
      " [0.08922545 0.60882965 0.24286798 0.05907691]]\n"
     ]
    }
   ],
   "source": [
    "# Importing\n",
    "import numpy as np\n",
    "import gym\n",
    "\n",
    "# Importing grid example as openAI gym\n",
    "import gym_gridworlds\n",
    "\n",
    "# Getting the environment that represent the example grid\n",
    "env = gym.make('Gridworld-v0')\n",
    "\n",
    "# States s = {s1 ... s14}\n",
    "S = env.observation_space\n",
    "\n",
    "# Actions A = {a1 ... 4}\n",
    "A = env.action_space\n",
    "\n",
    "# Transition function T or P that measures the prob\n",
    "# of given an action a_i taken, ending up in state s_j\n",
    "P = env.P\n",
    "\n",
    "# Reward function\n",
    "R = env.R\n",
    "\n",
    "# Starting random policy (or PDF for action to take being in a state)\n",
    "# Get random numbers and normalize to PDF in action axis \n",
    "pi = np.random.rand(S.n, A.n)\n",
    "pi = pi / np.sum(pi, axis=1, keepdims=True)\n",
    "\n",
    "# Random policy\n",
    "print(\"Pi initial policy:\")\n",
    "print(pi)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s_k0 = 1   # State 1\n",
    "a = 2      # down\n",
    "P_s_k1 = P[a][s_k0][:]\n",
    "\n",
    "# This should transition to state 5 with prob = 1\n",
    "P_s_k1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Evaluation\n",
    "\n",
    "TODO: Explain policy evaluation here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_evaluation(pi, S, A, P):\n",
    "    # Policy evaluation for being in each state\n",
    "    # V = {v(s1), ... , v(s14)}\n",
    "    V = np.zeros(S.n)\n",
    "    \n",
    "    # Convergence condition\n",
    "    theta = 0.001\n",
    "    \n",
    "    # Repeat till convergence\n",
    "    while True:\n",
    "        for s in range(S.n):\n",
    "            v = V[s]\n",
    "            # TODO!\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Improvement\n",
    "\n",
    "TODO: Explain policy improvement here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
